\documentclass{article}
\usepackage[utf8]{inputenc}

\title{khmer: Working with Big Data in Bioinformatics}
\author{Eric McDonald, Rosangela Canino-Koning, and C. Titus Brown}
\date{October 2012}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Bioinformatics and Big Data}

The field of bioinformatics seeks to understand the mechanisms which sustain and perpetuate life on Earth by examining information about the combination of and function of the molecules associated with these activities. The combinations and functions of these molecules are examined at different scales, the most common being nucleotides (the smallest) and proteins. A chemical and mechanical process, known as sequencing, extracts nucleotide sequences from the DNA and RNA present in terrestrial life. These sequences are recorded using an \textit{alphabet} of one letter per molecule. Various analyses are performed on this sequence data to determine how it is structured into larger building blocks and how it relates to other sequence data. This serves as the basis for the study of biological evolution and development, genetics, and the treatment of disease.

Data on nucleotide chains comes from the sequencing process in named strings of letters known as \textit{reads}. (The use of the term \textit{read} in the bioinformatic sense is an unfortunate collision with the use of the term in the computer science and software engineering sense. This is especially true as the performance of reading reads can be tuned, as we will discuss. We will try to disambiguate this unfortunate collision as much as we can in this chapter.) To analyze larger scale structures and processes, the nucleotide sequences of multiple reads must be fit together. This fitting is different than a jigsaw puzzle in that the picture is often not known a priori and that the pieces overlap one another. A further complication is introduced in that the reads are not perfect and may contain a variety of errors, such as insertions or deletions of nucleotides or representations of nucleotides with the wrong letters. While having redundant reads can help in the assembly or fitting of the puzzle pieces, it is also a hindrance because the various sequencing techniques all have a certain probability for producing errors and this means that the number of errors scales with the volume of data.

As sequencing technology has improved, the volume of sequence data being produced has begun to exceed the capabilities of computer hardware employing conventional methods for analyzing such data. This trend is expected to continue and is part of what is known as the \textit{Big Data} problem in the high performance computing (HPC) and analytics communities. With hardware becoming a limiting factor, increasing attention has turned to ways to mitigate the problem with software solutions. In this chapter, we present one such software solution and how we tuned it to efficiently handle terabytes of data.

\subsection{What is the khmer Software?}

Khmer, in addition to being an ethnic group indigenous to Southeast Asia, is the name of our suite of software tools for preprocessing large amounts of sequence data prior to its analysis with conventional tools. As part of the preprocessing the software performs, sequences are decomposed into overlapping substrings of a given length, \textit{k}. As chains of many molecules are often referred to as \textit{polymers}, chains of a specific number of molecules are called \textit{k-mers} in bioinformatics, each substring representing one such chain. (The hyphen between ``k'' and ``mer'' is a frequent convention and perhaps the ``h'' in the name of the software represents this.)

Since we want to tell you about how we measured and tuned this piece of open source software, we'll skip over much of the theory behind it. Suffice it to say that k-mer counting is central to much of its operation. To compactly count a large number of k-mers, a data structure known as a \textit{Bloom filter} is used. Armed with k-mer counts, we can then exclude highly redundant data from further processing. And, we can also treat low abundance sequence data as probable errors and exclude it from further processing as well. We call such exclusion or filtering \textit{digital normalization} and it is one of the major innovations that this software has lent to bioinformatics processing. This normalization process greatly reduces the amount of raw sequence data needed for further analysis while mostly preserving information of interest.

For the curious, the khmer sources and documentation can be cloned from GitHub at \texttt{http://github.com/ged-lab/khmer.git}.

\section{Architecture and Performance Considerations}

As the khmer software represents a novel theoretical approach to a portion of the Big Data problem in bioinformatics, the focus was on correctness during its initial implementation. Over time, as it came into greater use throughout the world, our attention became more focused on other issues such as performance and scalability. The various components of the software each have their own performance characteristics and challenges. Below, we examine these components.

Lots and lots of data (potentially terabytes) must be moved from disk to memory by the software. Having an efficient data pump is crucial, as the input throughput from storage may be 3 or even 4 orders of magnitude less than the throughput for data transfer involving volatile random-access memory. For some kinds of data files, a decompressor must be used. In either case, a parser must be able to work efficiently with the resultant data. The parsing task revolves around variable-length lines, but also must account for invalid reads (in the genomic sense) and what are known as paired reads. Each genomic read is broken up into a set of overlapping k-mers and each k-mer is registered with or compared against the Bloom filter. If a previously stored Bloom filter is being updated or used for comparison, then that must be loaded from storage. If a Bloom filter is being created for later use or updated, then it must be saved to storage.

The data pump always performs sequential access on files and can potentially be asked to read large chunks of data at one time. With this in mind, the following are some of the questions which come to mind:
\begin{itemize}
\item Are we fully exploiting the fact that the data is accessed sequentially?
\item Are enough pages of data being prefetched into memory to minimize access latency?
\item Can asynchronous input be used instead of synchronous input?
\item Can we efficiently bypass system caches to reduce buffer-to-buffer copies in memory?
\item Does the data pump expose data to the parser in a manner that does create any unnecessary accessor or decision logic overhead?
\end{itemize}

Some considerations, regarding parser efficiency, are:
\begin{itemize}
\item Have we minimized the number of times that the parser is touching the data in memory?
\item Have we minimized the number of buffer-to-buffer copies while parsing genomic reads from the data stream?
\item Have we minimized the function call overhead inside the parsing loop?
\item Is DNA sequence validation being done as efficiently as possible?
\end{itemize}

For iterating over the k-mers in a genomic read and hashing them, we could ask:
\begin{itemize}
\item Can the k-mer iteration mechanism be optimized for both memory and speed?
\item Can the Bloom filter hash functions be optimized in any way?
\item Have we minimized the number of times that the hasher is touching the data in memory?
\item Can we increment hash counts in batches to exploit a warm cache?
\end{itemize}

\section{Profiling and Measurement}

Although simple reading of the code revealed some areas which were clearly performance bottlenecks, we wanted to empirically identify and quantify where the problem spots were. (We're scientists and so its close to second nature for us to want to ``empirically identify and quantify'' things.) Using a combination of readily available open source tools and built-in instrumentation, we think that we got a fairly good idea of where the weakest performances were.

\subsection{Tools}

While we don't want to spend too much time on the specifics of the performance profiling tools which we used, we do want to give a shout out to these pieces of open source software and briefly mention how we used them.

The GNU Profiler, gprof, is one of the tools which we used. This profiler relies upon a compiler to instrument the source code and a linker to link in profiling versions of various libraries, as appropriate. When the compiled code is executed, it accumulates data about how much time is spent in particular functions. Upon completion of execution, this data is dumped to an output file, which can then be analyzed by the gprof tool proper.

% TODO: Write some more about how we used gprof.

We also used the Tuning and Analysis Utilities (TAU) from the University of Oregon. Whereas gprof instruments source code at compilation time, TAU offers many more profiling options. Not only can source be instrumented at the time of compilation, but library interposition can be used, in some cases, for the most non-intrusive means of profiling. Of course, library interposition has its limitations, and so TAU has other capabilities as well, such as the instrumentation of compiled executables by means of an execution wrapper. Naturally though, as one might expect, instrumentation of the source can provide the greatest level of detail and is necessary when working with OpenMP constructs, for example. Speaking of OpenMP, TAU can trace per-thread execution of parallel programs as parallelized by OpenMP and some other parallelism technologies. Message Passing Interface (MPI) is also well-supported, thereby allowing the profiling across multiple, separate address spaces when MPI is in use.

To use TAU to instrument your code, you need to use a compiler wrapper script, \texttt{tau\_cxx.sh}, provided with it. We had to integrate that wrapper script into our build system or else using it would have been too much of a nuisance. But, once past this and other barriers to entry (building TAU with the right set of options and dependencies, reading the manual, etc...), TAU proved to be quite useful.

% TODO: Write some more about how we used TAU.

\subsection{Manual Instrumentation}

Examining the performance of a piece of software with independent, external profilers is all well and good, but sometimes it is more convenient to get the software to tell you certain numbers itself. Also, manual instrumentation can be less intrusive than automatic instrumentation, since you directly control what gets observed. To this end, we created an extensible framework to internally measure things such as throughputs, iteration counts, and timings around atomic or fine-grained operations within the software itself. As a means of keeping us honest, we internally collected some numbers that could be compared with measurements from the external profilers.

% TODO: Write about the *PerformanceMetrics family of objects.

For different parts of the code, we needed to have different sets of metrics. However, all of the different sets of metrics have certain things in common. One thing is that they are mostly timing data and that you generally want to accumulate timings over the duration of execution. Another thing is that a consistent reporting mechanism is desirable. Given these considerations, we provided an abstract base class, \texttt{IPerformanceMetrics}, for all of our different sets of metrics. The \texttt{IPerformanceMetrics} class provides some convenience methods: \texttt{start\_timers}, \texttt{stop\_timers}, and \texttt{timespec\_diff\_in\_nsecs}. The methods for starting and stopping timers measure both elapsed real time and elapsed per-thread CPU time. The third method calculates the difference between two standard C library \texttt{timespec} objects in nanoseconds, which is of quite sufficient resolution for our purposes.

To ensure that the overhead of the manually-inserted internal instrumentation is not present in production code, we carefully wrapped it in conditional compilation directives so that a build can specify to exclude it.

\section{Tuning}

Making software work more efficiently is quite a gratifying experience, especially in the face of trillions of bytes passing through it. Our narrative will now wander through the various measures we took to improve efficiency. We divide these into two parts: optimization of the reading and parsing of input data and optimization of the manipulation and writing of the Bloom filter contents.

% TODO? Find a better home for the note on PGO.
Some compilers, including the GNU ones, can make use of the output from profilers to find certain optimizations to perform while they are compiling. This is known as \textit{profile-guided optimization} (PGO). At this point, profiler output has only been used a diagnostic to guide manual improvement of khmer's performance. We have not used PGO on khmer yet, but note it here for the sake of completeness. One reason that it has not been a high priority for us is that the availability and effectiveness of this technique varies from platform to platform and compiler suite to compiler suite. We would rather make improvements which are ubiquitously available.

\subsection{Data Pump and Parser Operations}

One of our major challenges with regards to the data pump is to maintain a large number of pages in memory and to keep them filled with fresh data. A number of operating systems, such as Linux, allow for some tuning of their readahead windows on a file-by-file basis. This gives some control over the situation, but not as much as we can actually gain in many cases. When a file residing on a local block device (a hard disk, for example) is accessed under Linux, you will often have the option of performing direct input. Direct input bypasses the standard system caches and moves file content directly from the block device into memory. This is different than typical input, where a copy of the contents is placed into a system cache with increased overhead. Since our access pattern is sequential and read-once, we do not need the contents to be cached - we simply need them to be made available for use as soon as possible.

Minimizing buffer-to-buffer copies is a challenge shared between the data pump and the parser. In the ideal scenario, we would like to read once from storage into our own cache and then read once from our own cache into the genomic read sequence string. However, the logic for managing the cache is complex enough and the logic for parsing (accounting for our particular nuances) is complex enough that maintaining an intermediary line buffer is quite useful for maintaining one's sanity while reading or debugging the code. To reduce the impact of this intermediary buffer, we encourage the compiler to fairly aggressively inline this portion of the code. We may ultimately opt to eliminate the intermediary buffer, but it will likely come at the expense of an understandable software design.

\subsection{Bloom Filter Operations}



\section{Parallelization}

% TODO: Refactor discussion. Some belongs before here.
With the proliferation of multi-core architectures in today's world, it is tempting to try taking advantage of them. However, unlike many other problem domains, such as computational fluid dynamics or molecular dynamics, our Big Data problem is essentially IO-bound. Beyond a certain point, throwing additional threads at it does not help as the bandwidth to the storage media is saturated and the threads simply end up with increased blocking or IO wait times. That said, utilizing some threads can be useful, particularly if the data to be processed is held in physical RAM, which generally has a much higher bandwidth than online storage. As part of our tuning, we took over cache management from the operating system so that we could efficiently make large blocks of data available in physical RAM. 

\subsection{Thread-safety and Threading}

People often confuse the notion of something being thread-safe with that of something being threaded. As part of our parallelization work, we remodeled portions of the C++ core implementation to be thread-safe without making any assumptions about a particular threading model. Therefore, the Python \texttt{threading} module can be used in the scripts which use the Python wrapper around the core implementation, or a C++ driver around the core could use a higher level abstraction, like OpenMP, or explicitly implement threading with \texttt{pthreads}, for example.

\subsection{Data Pump and Parser Operations}

The multi-core machines one encounters in the HPC world may have multiple memory controllers, where one controller is closer (in terms of signal travel distance) to one CPU than another CPU. These are Non-Uniform Memory Access (NUMA) architectures. A ramification of working with machines of this architecture is that memory fetch times may vary significantly depending on physical address. As bioinformatics software often requires a large memory footprint to run, it is often found running on these machines. Therefore, if one is using multiple threads, which may be pinned to various \textit{NUMA nodes}, the locality of the physical RAM must be taken into consideration. Part of our performance tuning consisted of allocating segments of our data cache in a properly localized manner.

\subsection{Bloom Filter Operations}

The Bloom filter hash tables cannot be split among threads. Rather, a single set of tables must be shared by all of the threads. This implies that there will be contention among the threads for these resources. Memory barriers or some form of locking are needed to prevent two or more threads from attempting to access the same memory location at the same time. We use atomic addition operations to increment the counters in the hash tables. These atomic operations are supported on a number of platforms by several compiler suites, the GNU compilers among those, and are not beholden to any particular threading model. They establish memory barriers around the operands which they are to update, thus adding thread-safety to a particular operation.

\section{Conclusion}

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=1.7]{universe.jpg}
%\caption{The Universe}
%\label{threadsVsSync}
%\end{figure}

%\section{Conclusion}
%``I always thought something was fundamentally wrong with the universe'' \citep{adams1995hitchhiker}

\bibliographystyle{plain}
\bibliography{references}
\end{document}

% vim: set ft=tex sw=4 sts=4:
